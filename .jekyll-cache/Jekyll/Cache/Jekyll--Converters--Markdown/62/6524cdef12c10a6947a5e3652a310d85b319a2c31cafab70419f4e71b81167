I"¿<h2 id="setting-the-stage">Setting the Stage</h2>

<!-- Let's suppose I have started recording all the days that it has rained this month where I live. At the end of the month, the recordings show that it has rained 15 out of the 30 days of the month. What can I say about the probability that it rained on any given day of that month?  -->

<p>There has recently been an outbreak of a dangerous new virus called OompaVirus! The worldâ€™s top scientists and reasearchers have been hard at work and have come up with the following statistics:</p>

<ul>
  <li>On any given day, there is a 30% chance that you contract OompaVirus</li>
  <li>If you have contracted OompaVirus, there is a 5% chance that you recover from it on any given day of you being sick</li>
</ul>

<p>As you scroll through the various headlines, you see that 20% of the population has already contracted OompaVirus. Your birthday is in 20 days and you want to know the probability that you will be healthy to celebrate it.</p>

<p>If you start solving this problem on paper, it may prove to be a bit harder than it initially seemed to be. At the start of every day, there are two possible outcomes: you either have the virus, or you donâ€™t. The tricky part about this problem is that each of those two outcomes depend only on your state the previous day. If you try to draw a tree of these possible outcomes after each day, you will start to find yourself lost in a binary tree that has a depth of 20. Trying to sum up the probability of each route and its outcome to get your answer is a tedious process. Our goal with Markov chains is to create a model of such a system and answer questions like this with simple equations.</p>

<h2 id="the-markov-chain">The Markov Chain</h2>

<!-- A Markov Chain is simply a model of a system in which something can change states over some unit time. In our example that 'something' would be the weather changing its state from rainy to sunny and our unit time would be one day The crux of this model lies in how we model changes in states. Instead of modeling them as fixed percentages, we model them as transitions of states. Instead of saying there is a 50% chance that it rained on any given day, we might say that every next day in that month, there was a 20% chance of transitioning from a rainy day to a sunny day, and an 80% chance of transitioning from a rainy day to a rainy day. 

We usually represent these Markov chains in two different ways: A transition diagram or a transition probability matrix. Let's first take a look at the former. Transition diagrams are really just directional graphs with the weights of edges representing the probability of transition from the first node to the second after 1 unit time. Therefore, the sum of all outbound edge weights should always be 1 (or 100%). -->

<p>All a Markov chain really does is model a system where the state of something changes after every unit time depending on its current state. In our case, the unit time is one day and our states are either healthy or infected. Generally, there are two ways we can represent a Markov chain: through a transition diagram, and through a transition probability matrix. Letâ€™s first take a look at the former.</p>

<p>A transition diagram is essentially a weighted directed graph with nodes representing states and edges representing transitions. The weights of these edges, therefore, represent the probability that the transition will occur after 1 unit time. The transition diagram for our example would look something like this:</p>

<p>IMAGE</p>

<p>This representation is very useful for understanding a Markov chain but it is not quite practical for mathematical operations. Thatâ€™s why mathematicians usually prefer matrix notation. We can condense this diagram into a matrix where the entry at \(i,j\) is the probability that we will transition from state \(i\) to state \(j\) after 1 unit time. We can create this matrix with the label \(v\) representing the infected state, and the label \(h\) representing the healthy state, like so:</p>

\[\begin{array}{c c c} &amp; &amp;

\begin{array}{c c} v &amp; h \\
\end{array} 
\\

P  = &amp;
\begin{array}{c c }
v \\
h
\end{array}
&amp;
\left[
\begin{array}{c c c}
0.95 &amp; 0.05  \\
0.3 &amp; 0.7 
\end{array}
\right]

\end{array}\]

<h2 id="state-distibution-matricies">State distibution Matricies</h2>

<!-- Now that we have a matrix representation of our markov chains, we can start doing all sorts of operations on it. Before we go any further though, I want to introduce the idea of a state distribution matrix. Remember that we were also given one more piece of information - the percent of global population infected at the start of 2056. Specifically, we were told that 20% of the global population had already been infected. We can create a matrix, $$S_0$$,  that represents this initial state distribution like so: -->

<p>Another matrix we will we will be using to solve this problem is known as the state distribution matrix. The \(n\)<sup>th</sup> state distribution matrix represents the probability that any given person is in one state or another, on day \(n\). In our case, that just means the portion of the population that infected on the \(n\)<sup>th</sup> day. Our zeroth state distribution matrix (AKA initial state distribution matrix) would therefore be â€¦</p>

\[\begin{array}{c} 

\begin{array}{c c}&amp;&amp;v &amp; h \\
\end{array} 
\\
S_0 = 
\left[
\begin{array}{c c }
0.2 &amp; 0.8 \\
\end{array}
\right]

\end{array}\]

<p>Now, if I want to find the portion of the population that is infected by tommorow, all I have to do is take the sum of the following:</p>
<ul>
  <li>the portion of healthy people multiplied by the probability that a healthy person contracts the virus at the end of one day</li>
  <li>the portion of infected people multiplied by the probability that an infected person remains infected at the end of one day</li>
</ul>

<p>Matrix multiplication allows us to do just that. If we multiply the \(n\)<sup>th</sup> state distribution matrix by the transition probability matrix, we will get the \(n+1\)<sup>th</sup> state distribution matrix.</p>

\[S_0 * P = S_1 \\



\left[
\begin{array}{c c }
0.2 &amp; 0.8 \\
\end{array}
\right]



\left[
\begin{array}{c c c}
0.95 &amp; 0.05  \\
0.3 &amp; 0.7 
\end{array}
\right]

= 

\left[
\begin{array}{c c }
0.43 &amp; 0.57 \\
\end{array}
\right]\]

<p>Going back to the question, letâ€™s see if there is a way to find the \(n\)<sup>th</sup> state distribution matrix using only the initial state distribution matrix. Since we know how to update our state distribution by one day, surely we can repeat the process to find the state distribution matrix of the \(n\)<sup>th</sup> day. Letâ€™s see how we would do this with \(n=4\).</p>

\[S_0 * P = S_1 \\
S_1 * P = S_2 \\

\therefore \\

(S_0 * P) * P = S_2 \\
((S_0 * P) * P) * P = S_3 \\
(((S_0 * P) * P) * P) * P= S_4\]

<p>This is no doubt a messy equation, but recall that matrix multiplication is associative (i.e. you can move the parenthesis around without changing the answer)</p>

\[(((S_0 * P) * P) * P ) * P = S_4\\
S_0 * (P*P*P*P) = S_4\\
S_0 * P^4 = S_4\]

<p>More generally,</p>

\[S_n = S_0 * P^n\]

<p>To answer our initial question,</p>

\[S_0 * P^{20} = S_20 \\

\left[
\begin{array}{c c }
0.2 &amp; 0.8 \\
\end{array}
\right]



\left[
\begin{array}{c c c}
0.857 &amp; 0.143  \\
0.857 &amp; 0.143 
\end{array}
\right]

= 

\left[
\begin{array}{c c }
0.857 &amp; 0.143 \\
\end{array}
\right]\]

<p>There is an 85.7% chance that we will be infected on the day of our birthday.</p>
:ET